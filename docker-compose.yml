services:
  chatbot:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./templates:/app/templates
    environment:
      - LLM_URL=http://llama-swap:8090
      - LLM_MODEL=qwen2.5-coder:32b
    restart: unless-stopped
    depends_on:
      - llama-swap

  llama-swap:
    build:
      context: ./llama-swap-docker
      dockerfile: Dockerfile
    image: chatbot/llama-swap
    container_name: llama-swap
    command: ["--config", "/config/llama-swap.yaml", "--listen", "0.0.0.0:8090"]
    volumes:
      - /home/lordmanuel/.llama/models:/models:ro
      - ./llama-swap.yaml:/config/llama-swap.yaml:ro
    runtime: nvidia
    ports:
      - "8090:8090"
    environment:
      - CUDA_VISIBLE_DEVICES=0
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]